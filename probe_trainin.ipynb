{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from src.train_test_utils import train, test \n",
    "import torch.nn as nn\n",
    "from src.dataset import TextDataset\n",
    "\n",
    "from src.probes import LinearProbeClassification\n",
    "import sklearn.model_selection\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Set your Hugging Face token as an environment variable\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"-------",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677bc7470c924982aca0ceacc38156a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ada44b6ef084b45bbb148039b0eb09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0faf51ff794af887e6203108152b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95df4d16068e40dca9a91add9b163f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/theo-env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6550b1811cbc4223b3b2647c3215fdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4898e390a14d25aaf735885dc3a14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc126b4626b45cab64784dedb5207c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58342e38bb254a8ca29474c6cf84296d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79eb71ec0624fc5a6b785df0a4208a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27a293f08654764a02de22f0c3f8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf2d20a64ed4da99a959a7dd3c81666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", use_auth_token=True)\n",
    "model.half().cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    learning_rate = 1e-3\n",
    "    betas = (0.9, 0.95)\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    # checkpoint settings\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unzip_files(zip_path, extract_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "# Unzip all necessary files\n",
    "'''\n",
    "for directory in '/root/usr_probes/dataset':\n",
    "    zip_path = directory[:-1] + '.zip'  # Assuming the zip file has the same name as the directory\n",
    "    if os.path.exists(zip_path):\n",
    "        unzip_files(zip_path, os.path.dirname(directory))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "jump_socioeco = True\n",
    "\n",
    "new_prompt_format=True\n",
    "residual_stream=True\n",
    "uncertainty = False\n",
    "logistic = True\n",
    "augmented = False\n",
    "remove_last_ai_response = True\n",
    "include_inst = True\n",
    "one_hot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id_age = {\"child\": 0,\n",
    "                   \"adolescent\": 1,\n",
    "                   \"adult\": 2,\n",
    "                   \"older adult\": 3,\n",
    "                  }\n",
    "\n",
    "label_to_id_gender = {\"male\": 0,\n",
    "                      \"female\": 1,\n",
    "                     }\n",
    "\n",
    "label_to_id_socioeconomic = {\"low\": 0,\n",
    "                             \"middle\": 1,\n",
    "                             \"high\": 2}\n",
    "\n",
    "label_to_id_neweducation = {\"someschool\": 0,\n",
    "                            \"highschool\": 1,\n",
    "                            \"collegemore\": 2}\n",
    "\n",
    "prompt_translator = {\"_age_\": \"age\",\n",
    "                     \"_gender_\": \"gender\",\n",
    "                     \"_socioeco_\": \"socioeconomic status\",\n",
    "                     \"_education_\": \"education level\",}\n",
    "\n",
    "openai_dataset = {\"_age_\": \"dataset/openai_age_1/\",\n",
    "                  \"_gender_\": \"dataset/openai_gender_1/\",\n",
    "                  \"_education_\": \"dataset/openai_education_1/\",\n",
    "                  \"_socioeco_\": \"dataset/openai_socioeconomic_1/\",}\n",
    "\n",
    "accuracy_dict = {}\n",
    "\n",
    "directories = [\"/root/usr_probes/dataset/llama_age_1/\", \"/root/usr_probes/dataset/llama_gender_1/\",\n",
    "               \"/root/usr_probes/dataset/llama_socioeconomic_1/\", \"/root/usr_probes/dataset/openai_education_1/\"]\n",
    "\n",
    "label_idfs = [\"_age_\", \"_gender_\", \"_socioeco_\", \"_education_\"]\n",
    "\n",
    "label_to_ids = [label_to_id_age, label_to_id_gender,\n",
    "                label_to_id_socioeconomic, label_to_id_neweducation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THEO\n",
      "THEO\n",
      "THEO\n",
      "Warning: Neither directory /root/usr_probes/dataset/openai_education_1/ nor zip file /root/usr_probes/dataset/openai_education_1.zip found. Skipping...\n"
     ]
    }
   ],
   "source": [
    "for directory, label_idf, label_to_id in zip(directories, label_idfs, label_to_ids):\n",
    "    if not os.path.exists(directory):\n",
    "        zip_path = directory[:-1] + '.zip'\n",
    "        if os.path.exists(zip_path):\n",
    "            unzip_files(zip_path, os.path.dirname(directory))\n",
    "        else:\n",
    "            print(f\"Warning: Neither directory {directory} nor zip file {zip_path} found. Skipping...\")\n",
    "            continue\n",
    "    # additional_dataset=[directory[:-1] + \"_additional/\"]\n",
    "    if label_idf == \"_education_\":\n",
    "        additional_dataset=[]\n",
    "    else:\n",
    "        print('THEO')\n",
    "        continue\n",
    "        #additional_dataset=[directory[:-2] + \"2.zip\", openai_dataset[label_idf]]\n",
    "    if label_idf == \"_gender_\":\n",
    "        additional_dataset += [\"/root/usr_probes/dataset/openai_gender_2/\", \"/root/usr_probes/dataset/openai_gender_3/\", \n",
    "                               \"/root/usr_probes/dataset/openai_gender_4\",]\n",
    "    if label_idf == \"_education_\":\n",
    "        additional_dataset += [\"/root/usr_probes/dataset/openai_education_2\", \"/root/usr_probes/dataset/openai_education_3/\"]\n",
    "    if label_idf == \"_socioeco_\":\n",
    "        additional_dataset += [\"/root/usr_probes/dataset/openai_socioeconomic_2/\", \"/root/usr_probes/dataset/openai_socioeconomic_3/\"]\n",
    "    if label_idf == \"_age_\":\n",
    "        additional_dataset += [\"/root/usr_probes/dataset/openai_age_2/\", \"/root/usr_probes/dataset/openai_age_3/\"]\n",
    "        \n",
    "    dataset = TextDataset(directory, tokenizer, model, label_idf=label_idf, label_to_id=label_to_id,\n",
    "                          convert_to_llama2_format=True, additional_datas=additional_dataset, \n",
    "                          new_format=new_prompt_format,\n",
    "                          residual_stream=residual_stream, if_augmented=augmented, \n",
    "                          remove_last_ai_response=remove_last_ai_response, include_inst=include_inst, k=1,\n",
    "                          one_hot=False, last_tok_pos=-1)\n",
    "    dict_name = label_idf.strip(\"_\")\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_idx, val_idx = sklearn.model_selection.train_test_split(list(range(len(dataset))), \n",
    "                                                                  test_size=test_size,\n",
    "                                                                  train_size=train_size,\n",
    "                                                                  random_state=12345,\n",
    "                                                                  shuffle=True,\n",
    "                                                                  stratify=dataset.labels,\n",
    "                                                                 )\n",
    "\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    test_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "    sampler = None\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, sampler=sampler, pin_memory=True, batch_size=200, num_workers=1)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, pin_memory=True, batch_size=400, num_workers=1)\n",
    "\n",
    "    loss_func = nn.BCELoss()\n",
    "    torch_device = \"cuda\"\n",
    "\n",
    "    seeds = seeds[:9]\n",
    "    accuracy_dict[dict_name] = []\n",
    "    accuracy_dict[dict_name + \"_final\"] = []\n",
    "    accuracy_dict[dict_name + \"_train\"] = []\n",
    "        \n",
    "    accs = []\n",
    "    final_accs = []\n",
    "    train_accs = []\n",
    "    for i in tqdm(range(0, 41)):\n",
    "        trainer_config = TrainerConfig()\n",
    "        probe = LinearProbeClassification(probe_class=len(label_to_id.keys()), device=\"cuda\", input_dim=5120,\n",
    "                                            logistic=logistic)\n",
    "        optimizer, scheduler = probe.configure_optimizers(trainer_config)\n",
    "        best_acc = 0\n",
    "        max_epoch = 50\n",
    "        verbosity = False\n",
    "        layer_num = i\n",
    "        print(\"-\" * 40 + f\"Layer {layer_num}\" + \"-\" * 40)\n",
    "        for epoch in range(1, max_epoch + 1):\n",
    "            if epoch == max_epoch:\n",
    "                verbosity = True\n",
    "            # Get the train results from training of each epoch\n",
    "            if uncertainty:\n",
    "                train_results = train(probe, torch_device, train_loader, optimizer, \n",
    "                                        epoch, loss_func=loss_func, verbose_interval=None,\n",
    "                                        verbose=verbosity, layer_num=layer_num, \n",
    "                                        return_raw_outputs=True, epoch_num=epoch, num_classes=len(label_to_id.keys()))\n",
    "                test_results = test(probe, torch_device, test_loader, loss_func=loss_func, \n",
    "                                    return_raw_outputs=True, verbose=verbosity, layer_num=layer_num,\n",
    "                                    scheduler=scheduler, epoch_num=epoch, num_classes=len(label_to_id.keys()))\n",
    "            else:\n",
    "                train_results = train(probe, torch_device, train_loader, optimizer, \n",
    "                                        epoch, loss_func=loss_func, verbose_interval=None,\n",
    "                                        verbose=verbosity, layer_num=layer_num,\n",
    "                                        return_raw_outputs=True,\n",
    "                                        one_hot=one_hot, num_classes=len(label_to_id.keys()))\n",
    "                test_results = test(probe, torch_device, test_loader, loss_func=loss_func, \n",
    "                                    return_raw_outputs=True, verbose=verbosity, layer_num=layer_num,\n",
    "                                    scheduler=scheduler,\n",
    "                                    one_hot=one_hot, num_classes=len(label_to_id.keys()))\n",
    "\n",
    "            if test_results[1] > best_acc:\n",
    "                best_acc = test_results[1]\n",
    "                torch.save(probe.state_dict(), f\"probe_checkpoints/reading_probe/{dict_name}_probe_at_layer_{layer_num}.pth\")\n",
    "        torch.save(probe.state_dict(), f\"probe_checkpoints/reading_probe/{dict_name}_probe_at_layer_{layer_num}_final.pth\")\n",
    "        \n",
    "        accs.append(best_acc)\n",
    "        final_accs.append(test_results[1])\n",
    "        train_accs.append(train_results[1])\n",
    "        cm = confusion_matrix(test_results[3], test_results[2])\n",
    "        cm_display = ConfusionMatrixDisplay(cm, display_labels=label_to_id.keys()).plot()\n",
    "        plt.show()\n",
    "\n",
    "        accuracy_dict[dict_name].append(accs)\n",
    "        accuracy_dict[dict_name + \"_final\"].append(final_accs)\n",
    "        accuracy_dict[dict_name + \"_train\"].append(train_accs)\n",
    "        \n",
    "        with open(\"probe_checkpoints/reading_probe_experiment.pkl\", \"wb\") as outfile:\n",
    "            pickle.dump(accuracy_dict, outfile)\n",
    "    del dataset, train_dataset, test_dataset, train_loader, test_loader\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
